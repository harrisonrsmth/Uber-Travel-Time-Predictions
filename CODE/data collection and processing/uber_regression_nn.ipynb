{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          distance  day_of_week  hour_bucket      temp    precip  wind_speed  \\\n",
      "0        -0.179978    -0.612504    -0.592143 -0.646645 -0.482898   -0.411618   \n",
      "1        -0.176209    -0.515786    -0.393406 -0.639491  0.185643   -0.393303   \n",
      "2        -0.176209    -0.515786    -0.312704 -0.639523  0.082063   -0.389406   \n",
      "3        -0.176209    -0.498451    -0.554809 -0.638728 -0.041466   -0.399390   \n",
      "4        -0.176209    -0.498451    -0.474108 -0.638044  0.318381   -0.397686   \n",
      "...            ...          ...          ...       ...       ...         ...   \n",
      "12168148 -0.010657    -0.138057    -0.430459 -0.006315  0.003082   -0.128694   \n",
      "12168149 -0.010657    -0.269445     0.128419 -0.077553  0.052546   -0.015224   \n",
      "12168150 -0.010657    -0.138057     0.128419  0.127475 -0.013168   -0.141301   \n",
      "12168151 -0.010657    -0.269445    -0.430459 -0.016957 -0.013168   -0.090082   \n",
      "12168152 -0.010657    -0.269445    -0.430459 -0.092975 -0.013168   -0.167306   \n",
      "\n",
      "          mean_travel_time  \n",
      "0                  1090.14  \n",
      "1                  1465.83  \n",
      "2                  1362.29  \n",
      "3                  1108.86  \n",
      "4                  1489.67  \n",
      "...                    ...  \n",
      "12168148            979.67  \n",
      "12168149            819.33  \n",
      "12168150            859.50  \n",
      "12168151            718.14  \n",
      "12168152            952.00  \n",
      "\n",
      "[12168153 rows x 7 columns]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.utils.data.dataset.Subset'>\n",
      "9734522\n",
      "2433631\n"
     ]
    }
   ],
   "source": [
    "#load dataset and init dataloader\n",
    "df = pd.read_parquet(\"final_data_parquet.parquet\")\n",
    "print(df)\n",
    "data = df.to_numpy()\n",
    "\n",
    "# x_numpy = data[:,np.r_[0:2, 5:10]]\n",
    "x_numpy = data[:,0:6]\n",
    "y_numpy = data[:,6]\n",
    "# print(x_numpy)\n",
    "\n",
    "print(type(x_numpy))\n",
    "print(type(y_numpy))\n",
    "\n",
    "x_tensor = torch.from_numpy(x_numpy.astype(np.float32))\n",
    "y_tensor = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "\n",
    "print(type(x_tensor))\n",
    "print(type(y_tensor))\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 4096\n",
    "train_DL = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "print(type(train_dataset))\n",
    "\n",
    "print(train_size)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: torch.Size([4096, 6])\n",
      "Targets: torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "#show sample\n",
    "sample = next(iter(train_DL))\n",
    "print('Inputs:', sample[0].shape)\n",
    "print('Targets:', sample[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num features\n",
    "nf = 64\n",
    "\n",
    "#use GPU if available \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Build convolutional neural network\n",
    "class Uber_reg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Uber_reg, self).__init__() #init Uber_reg's superclass\n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(6, 64), #out 64 features\n",
    "            # nn.BatchNorm2d(nf), \n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            \n",
    "            nn.Linear(64, 32), #out 32\n",
    "            # nn.BatchNorm2d(nf*2), #same as output channels\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            \n",
    "            nn.Linear(32, 16), #out 16\n",
    "            # nn.BatchNorm2d(nf*4),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            \n",
    "            nn.Linear(16, 8), #out 8\n",
    "            # nn.BatchNorm2d(nf*2),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            \n",
    "            nn.Linear(8, 4), #out 4\n",
    "            # nn.BatchNorm2d(nf),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "            nn.Linear(4, 2), #out 2\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "            nn.Linear(2, 1), #out 1\n",
    "            nn.LeakyReLU(0.01, inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(1, x.shape)\n",
    "        #reshape for dummy data with 1 feature\n",
    "        # x = x.reshape((x.shape[0], 1))\n",
    "        # print(2, x.shape)\n",
    "        x = self.lin(x)\n",
    "        # print(3, x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uber_reg(\n",
      "  (lin): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (6): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (8): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (10): Linear(in_features=4, out_features=2, bias=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (12): Linear(in_features=2, out_features=1, bias=True)\n",
      "    (13): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize and test network\n",
    "net = Uber_reg().to(device)\n",
    "print(net)\n",
    "net(sample[0].to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize error function and optimizer\n",
    "opt = torch.optim.Adam(net.parameters(), betas=(0.9, 0.999), lr = 0.001)\n",
    "error = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize test dataloader\n",
    "batch_size = 256\n",
    "test_DL = torch.utils.data.DataLoader(test_dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING     Epoch: 1/10     Iter: 0     Loss: 1049452.25\n",
      "TRAINING     Epoch: 1/10     Iter: 1000     Loss: 342071.27764423075\n",
      "TRAINING     Epoch: 1/10     Iter: 2000     Loss: 225091.9556784108\n",
      "TESTING      Epoch: 1/10     Iter: 3000     Loss: 104813.28486202925\n",
      "TESTING      Epoch: 1/10     Iter: 4000     Loss: 104864.43297558113\n",
      "TESTING      Epoch: 1/10     Iter: 5000     Loss: 104489.84835145532\n",
      "TESTING      Epoch: 1/10     Iter: 6000     Loss: 104639.75013473544\n",
      "TESTING      Epoch: 1/10     Iter: 7000     Loss: 104515.50754217128\n",
      "TESTING      Epoch: 1/10     Iter: 8000     Loss: 104575.43937602796\n",
      "TESTING      Epoch: 1/10     Iter: 9000     Loss: 104587.3672989555\n",
      "TESTING      Epoch: 1/10     Iter: 10000     Loss: 104596.36232569435\n",
      "TESTING      Epoch: 1/10     Iter: 11000     Loss: 104524.84192868376\n",
      "TRAINING     Epoch: 2/10     Iter: 12000     Loss: 201321.20824040196\n",
      "TRAINING     Epoch: 2/10     Iter: 13000     Loss: 173150.49280462938\n",
      "TRAINING     Epoch: 2/10     Iter: 14000     Loss: 157075.3944234674\n",
      "TESTING      Epoch: 2/10     Iter: 15000     Loss: 104118.99002566299\n",
      "TESTING      Epoch: 2/10     Iter: 16000     Loss: 103792.24271923902\n",
      "TESTING      Epoch: 2/10     Iter: 17000     Loss: 103519.67171807382\n",
      "TESTING      Epoch: 2/10     Iter: 18000     Loss: 103134.93848459792\n",
      "TESTING      Epoch: 2/10     Iter: 19000     Loss: 102905.5156729816\n",
      "TESTING      Epoch: 2/10     Iter: 20000     Loss: 102653.47912755379\n",
      "TESTING      Epoch: 2/10     Iter: 21000     Loss: 102426.72484583693\n",
      "TESTING      Epoch: 2/10     Iter: 22000     Loss: 102249.78155440076\n",
      "TESTING      Epoch: 2/10     Iter: 23000     Loss: 102060.06404134927\n",
      "TRAINING     Epoch: 3/10     Iter: 24000     Loss: 151381.1378960297\n",
      "TRAINING     Epoch: 3/10     Iter: 25000     Loss: 142566.58401536662\n",
      "TRAINING     Epoch: 3/10     Iter: 26000     Loss: 136119.42886007047\n",
      "TESTING      Epoch: 3/10     Iter: 27000     Loss: 101768.71264488708\n",
      "TESTING      Epoch: 3/10     Iter: 28000     Loss: 101560.53131588404\n",
      "TESTING      Epoch: 3/10     Iter: 29000     Loss: 101360.14357531721\n",
      "TESTING      Epoch: 3/10     Iter: 30000     Loss: 101173.02258946627\n",
      "TESTING      Epoch: 3/10     Iter: 31000     Loss: 100980.50132913962\n",
      "TESTING      Epoch: 3/10     Iter: 32000     Loss: 100810.14520255328\n",
      "TESTING      Epoch: 3/10     Iter: 33000     Loss: 100661.46403683079\n",
      "TESTING      Epoch: 3/10     Iter: 34000     Loss: 100532.20870496256\n",
      "TESTING      Epoch: 3/10     Iter: 35000     Loss: 100418.77089444295\n",
      "TRAINING     Epoch: 4/10     Iter: 36000     Loss: 133563.52089112633\n",
      "TRAINING     Epoch: 4/10     Iter: 37000     Loss: 129071.04695330925\n",
      "TRAINING     Epoch: 4/10     Iter: 38000     Loss: 125371.13301770174\n",
      "TESTING      Epoch: 4/10     Iter: 39000     Loss: 100115.0422560544\n",
      "TESTING      Epoch: 4/10     Iter: 40000     Loss: 99919.05905389659\n",
      "TESTING      Epoch: 4/10     Iter: 41000     Loss: 99693.37877315991\n",
      "TESTING      Epoch: 4/10     Iter: 42000     Loss: 99485.4546092139\n",
      "TESTING      Epoch: 4/10     Iter: 43000     Loss: 99312.80325891325\n",
      "TESTING      Epoch: 4/10     Iter: 44000     Loss: 99144.45877820771\n",
      "TESTING      Epoch: 4/10     Iter: 45000     Loss: 98970.55540033862\n",
      "TESTING      Epoch: 4/10     Iter: 46000     Loss: 98800.19831853455\n",
      "TESTING      Epoch: 4/10     Iter: 47000     Loss: 98650.75813464346\n",
      "TRAINING     Epoch: 5/10     Iter: 48000     Loss: 123775.71380383286\n",
      "TRAINING     Epoch: 5/10     Iter: 49000     Loss: 120921.07031321198\n",
      "TESTING      Epoch: 5/10     Iter: 50000     Loss: 98557.72129681022\n",
      "TESTING      Epoch: 5/10     Iter: 51000     Loss: 98391.50165593204\n",
      "TESTING      Epoch: 5/10     Iter: 52000     Loss: 98232.52049125894\n",
      "TESTING      Epoch: 5/10     Iter: 53000     Loss: 98068.19102537258\n",
      "TESTING      Epoch: 5/10     Iter: 54000     Loss: 97905.07678838282\n",
      "TESTING      Epoch: 5/10     Iter: 55000     Loss: 97769.18892411968\n",
      "TESTING      Epoch: 5/10     Iter: 56000     Loss: 97629.27871354073\n",
      "TESTING      Epoch: 5/10     Iter: 57000     Loss: 97500.18568280932\n",
      "TESTING      Epoch: 5/10     Iter: 58000     Loss: 97376.3848663391\n",
      "TESTING      Epoch: 5/10     Iter: 59000     Loss: 97241.56921340084\n",
      "TRAINING     Epoch: 6/10     Iter: 60000     Loss: 117439.53374052423\n",
      "TRAINING     Epoch: 6/10     Iter: 61000     Loss: 115501.68485966972\n",
      "TESTING      Epoch: 6/10     Iter: 62000     Loss: 97159.61704479696\n",
      "TESTING      Epoch: 6/10     Iter: 63000     Loss: 97025.88904371372\n",
      "TESTING      Epoch: 6/10     Iter: 64000     Loss: 96898.91814278346\n",
      "TESTING      Epoch: 6/10     Iter: 65000     Loss: 96771.02614336666\n",
      "TESTING      Epoch: 6/10     Iter: 66000     Loss: 96650.56772793795\n",
      "TESTING      Epoch: 6/10     Iter: 67000     Loss: 96552.9193350746\n",
      "TESTING      Epoch: 6/10     Iter: 68000     Loss: 96441.16957358425\n",
      "TESTING      Epoch: 6/10     Iter: 69000     Loss: 96343.06836319284\n",
      "TESTING      Epoch: 6/10     Iter: 70000     Loss: 96246.34154265583\n",
      "TESTING      Epoch: 6/10     Iter: 71000     Loss: 96150.79290224482\n",
      "TRAINING     Epoch: 7/10     Iter: 72000     Loss: 113050.66085372937\n",
      "TRAINING     Epoch: 7/10     Iter: 73000     Loss: 111653.729642169\n",
      "TESTING      Epoch: 7/10     Iter: 74000     Loss: 96099.36369678532\n",
      "TESTING      Epoch: 7/10     Iter: 75000     Loss: 95997.51670948937\n",
      "TESTING      Epoch: 7/10     Iter: 76000     Loss: 95895.40108511916\n",
      "TESTING      Epoch: 7/10     Iter: 77000     Loss: 95806.8154398152\n",
      "TESTING      Epoch: 7/10     Iter: 78000     Loss: 95724.0926292178\n",
      "TESTING      Epoch: 7/10     Iter: 79000     Loss: 95629.10552199256\n",
      "TESTING      Epoch: 7/10     Iter: 80000     Loss: 95541.46893747682\n",
      "TESTING      Epoch: 7/10     Iter: 81000     Loss: 95462.6584509906\n",
      "TESTING      Epoch: 7/10     Iter: 82000     Loss: 95384.8434370195\n",
      "TESTING      Epoch: 7/10     Iter: 83000     Loss: 95318.24326033253\n",
      "TRAINING     Epoch: 8/10     Iter: 84000     Loss: 109832.48888513852\n",
      "TRAINING     Epoch: 8/10     Iter: 85000     Loss: 108769.66255267044\n",
      "TESTING      Epoch: 8/10     Iter: 86000     Loss: 95271.98790470815\n",
      "TESTING      Epoch: 8/10     Iter: 87000     Loss: 95191.98479920892\n",
      "TESTING      Epoch: 8/10     Iter: 88000     Loss: 95124.35001659102\n",
      "TESTING      Epoch: 8/10     Iter: 89000     Loss: 95055.52208051279\n",
      "TESTING      Epoch: 8/10     Iter: 90000     Loss: 94989.18374425494\n",
      "TESTING      Epoch: 8/10     Iter: 91000     Loss: 94909.26606769205\n",
      "TESTING      Epoch: 8/10     Iter: 92000     Loss: 94835.92607278036\n",
      "TESTING      Epoch: 8/10     Iter: 93000     Loss: 94776.32745626225\n",
      "TESTING      Epoch: 8/10     Iter: 94000     Loss: 94710.50461217243\n",
      "TESTING      Epoch: 8/10     Iter: 95000     Loss: 94650.58853777678\n",
      "TRAINING     Epoch: 9/10     Iter: 96000     Loss: 107364.93267422913\n",
      "TRAINING     Epoch: 9/10     Iter: 97000     Loss: 106535.38284159405\n",
      "TESTING      Epoch: 9/10     Iter: 98000     Loss: 94607.00456682681\n",
      "TESTING      Epoch: 9/10     Iter: 99000     Loss: 94554.60864929759\n",
      "TESTING      Epoch: 9/10     Iter: 100000     Loss: 94492.4186521251\n",
      "TESTING      Epoch: 9/10     Iter: 101000     Loss: 94431.46924613223\n",
      "TESTING      Epoch: 9/10     Iter: 102000     Loss: 94375.01809334992\n",
      "TESTING      Epoch: 9/10     Iter: 103000     Loss: 94323.8130213088\n",
      "TESTING      Epoch: 9/10     Iter: 104000     Loss: 94273.1386060048\n",
      "TESTING      Epoch: 9/10     Iter: 105000     Loss: 94218.0216031398\n",
      "TESTING      Epoch: 9/10     Iter: 106000     Loss: 94168.96515252694\n",
      "TRAINING     Epoch: 10/10     Iter: 107000     Loss: 106150.55012127997\n",
      "TRAINING     Epoch: 10/10     Iter: 108000     Loss: 105423.79208959131\n",
      "TRAINING     Epoch: 10/10     Iter: 109000     Loss: 104735.04455538282\n",
      "TESTING      Epoch: 10/10     Iter: 110000     Loss: 94081.7283113001\n",
      "TESTING      Epoch: 10/10     Iter: 111000     Loss: 94022.79181041215\n",
      "TESTING      Epoch: 10/10     Iter: 112000     Loss: 93974.4748456378\n",
      "TESTING      Epoch: 10/10     Iter: 113000     Loss: 93925.08930012937\n",
      "TESTING      Epoch: 10/10     Iter: 114000     Loss: 93885.37089014931\n",
      "TESTING      Epoch: 10/10     Iter: 115000     Loss: 93839.52886628448\n",
      "TESTING      Epoch: 10/10     Iter: 116000     Loss: 93799.46270916401\n",
      "TESTING      Epoch: 10/10     Iter: 117000     Loss: 93744.85863012639\n",
      "TESTING      Epoch: 10/10     Iter: 118000     Loss: 93699.33307526849\n"
     ]
    }
   ],
   "source": [
    "#training parameters\n",
    "epochs = 10\n",
    "iters = 0\n",
    "iters_cycle = 1000\n",
    "\n",
    "#save losses\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_avg_losses = []\n",
    "test_avg_losses = []\n",
    "\n",
    "#train\n",
    "for epoch in range(epochs):\n",
    "    #set model to training mode\n",
    "    net.train()\n",
    "    #iterate through dataloader\n",
    "    for batch in train_DL:\n",
    "        #separate batch into targets and inputs\n",
    "        inputs = batch[0].to(device)\n",
    "        targets = batch[1]\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "                \n",
    "        #make predictions\n",
    "        pred = net(inputs)\n",
    "        \n",
    "        #calculate loss\n",
    "        loss = error(pred, targets)\n",
    "        \n",
    "        #backpropagate gradients with Adam algorithm\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        #set gradients to zero\n",
    "        net.zero_grad()\n",
    "        \n",
    "        #save losses\n",
    "        train_losses.append(loss.item())\n",
    "        train_avg_losses.append(np.mean(train_losses))\n",
    "        \n",
    "        #log progress\n",
    "        if iters % iters_cycle == 0:    \n",
    "            print('TRAINING     Epoch: {}/{}     Iter: {}     Loss: {}'.format(epoch + 1, epochs, iters, np.mean(train_losses)))\n",
    "        iters +=1\n",
    "\n",
    "    #set model to evaluation mode\n",
    "    net.eval()\n",
    "    for batch in test_DL:\n",
    "        #separate batch into targets and inputs\n",
    "        inputs = batch[0].to(device)\n",
    "        targets = batch[1].to(device)\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "                \n",
    "        #make predictions\n",
    "        with torch.no_grad():\n",
    "            pred = net(inputs)\n",
    "        \n",
    "        #calculate loss\n",
    "        opt.zero_grad()\n",
    "        loss = error(pred, targets)\n",
    "        \n",
    "        #save losses\n",
    "        test_losses.append(loss.item())\n",
    "        test_avg_losses.append(np.mean(test_losses))\n",
    "        \n",
    "        #log progress\n",
    "        if iters % iters_cycle == 0:    \n",
    "            print('TESTING      Epoch: {}/{}     Iter: {}     Loss: {}'.format(epoch + 1, epochs, iters, np.mean(test_losses)))\n",
    "        iters +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ffb619f7730>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot loss curve \n",
    "# plt.plot(train_avg_losses)\n",
    "plt.plot(test_avg_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the path to save/load the model\n",
    "model_dir = 'checkpoints/'\n",
    "\n",
    "def save_model(model_dir):\n",
    "    #saves the model state and optimizer state on the dict\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    torch.save({\n",
    "        'model_state_dict': net.state_dict(),\n",
    "        'optimizer_state_dict': opt.state_dict()\n",
    "    }, os.path.join(model_dir, 'checkpoint.pt'))\n",
    "    print('model saved at', model_dir + 'checkpoint.pt')\n",
    "\n",
    "def load_model(model_dir, load_optimizer=False):\n",
    "    #load the model from the disk if it exists, skip if you don't need this part\n",
    "    if os.path.exists(model_dir):\n",
    "        checkpoint = torch.load(os.path.join(model_dir, 'checkpoint.pt'))\n",
    "        net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        if load_optimizer:\n",
    "            opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print('loaded model from saved checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved at checkpoints/checkpoint4.pt\n"
     ]
    }
   ],
   "source": [
    "save_model(model_dir)\n",
    "# load_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate accuracy using predictions\n",
    "def testAccuracy(model, test_set, cpu = 'False'):\n",
    "    #set device\n",
    "    if cpu == True:\n",
    "        dev = torch.device('cpu')\n",
    "    else:\n",
    "        dev = torch.device('cuda:0')\n",
    "    model = model.to(dev)\n",
    "    test_inputs = test_set[0].to(dev)\n",
    "    test_targets = test_set[1].to(dev)\n",
    "    \n",
    "    #make predictions\n",
    "    with torch.no_grad(): \n",
    "        model_pred = model(test_inputs).cpu().numpy() \n",
    "        \n",
    "    #Count number of correct predictions    \n",
    "    correct = 0\n",
    "    total = test_targets.shape[0]\n",
    "    for i, prediction in enumerate(model_pred):\n",
    "        if test_targets[i]==np.argmax(prediction):\n",
    "            correct += 1\n",
    "            \n",
    "    #revert model to global device\n",
    "    model=model.to(device)\n",
    "    \n",
    "    #print accuracy\n",
    "    return print('Accuracy across {} samples: {}%'.format(test_targets.shape[0], correct/total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy across 50 samples: 0.0%\n"
     ]
    }
   ],
   "source": [
    "#Evaluate accuracy on test set\n",
    "test_set = next(iter(test_DL))\n",
    "testAccuracy(net, test_set, cpu = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 1212.0\n",
      "Prediction: 997.8786010742188\n"
     ]
    }
   ],
   "source": [
    "#display sample with label and prediction\n",
    "sample = next(iter(test_DL))\n",
    "print('Target:', sample[1][0].item())\n",
    "print('Prediction:', net(sample[0][0].to(device))[0].item())\n",
    "# plt.imshow(sample[0][0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "907.6471557617188\n"
     ]
    }
   ],
   "source": [
    "test_sample = [130, 24, 2020, 3, 12, 5, 1, 63.25, 0, 5.25, 0, 0]\n",
    "test_pred = np.array([test_sample])\n",
    "test_pred_tensor = torch.from_numpy(test_pred.astype(np.float32))\n",
    "prediction = net(test_pred_tensor.to(device))[0].item()\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
